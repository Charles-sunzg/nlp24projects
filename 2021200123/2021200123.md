# 项目报告：基于GPT2中文大模型的古诗词生成模型
孙志钢 2021200123
## 1. 研究背景和研究目标
### 1.1 研究背景
古诗词作为中国传统文化的瑰宝，具有深厚的历史背景和广泛的文化影响力。它们不仅是文学艺术的表现形式，也承载了丰富的哲学思想、历史故事、情感表达等内容。

随着人工智能和自然语言处理（NLP）技术的飞速发展，深度学习模型，特别是基于神经网络的生成模型，已经在多个领域取得了突破。近年来，GPT系列模型、BERT、Transformer等技术的出现使得语言生成任务，尤其是文学创作，进入了新的阶段。

尽管现代技术在诗词创作方面已取得一定进展，但由于古诗词具有独特的韵律、对仗、意境等要求，古诗词的自动生成依然面临诸多挑战。许多早期的模型未能很好地解决语言流畅性、文化底蕴与创意之间的平衡问题。因此，如何利用深度学习技术生成具有较高文学价值的古诗词，成为了一个重要的研究方向。

### 1.2 研究目的
目前的自动古诗词生成模型面临的问题包括语法结构不准确、韵律不和谐、情感表达不到位等。研究的目的之一是通过改进模型架构，优化训练数据，增强模型对古诗词特征的学习能力，提高生成诗词的质量与准确性，使其更具文学性与艺术性。

## 2. 数据准备
本项目采用中华诗词数据库，数据库链接：[诗词数据库](https://github.com/chinese-poetry/chinese-poetry)

此数据库是最全的中华古典文集数据库，包含 5.5 万首唐诗、26 万首宋诗、2.1 万首宋词和其他古典文集。诗人包括唐宋两朝近 1.4 万古诗人，和两宋时期 1.5 千古词人。数据来源于互联网。

本项目提取其中的5.5 万首唐诗、26 万首宋诗、2.1 万首宋词作为训练集进行训练，利用数据库进行基本操作后转化为csv。详细路径见[数据](./src/data_path.md)

## 3. LSTM模型尝试
本次首先采用LSTM模型进行初步的生成模型训练，主要代码见src下old version文件夹

### 3.1 数据处理
由于数据库主要为繁体字，首先调用hanziconv库进行简单的简繁转换。

首先，利用函数读取并清洗数据，去除缺失值，过滤掉包含禁用词或过长的诗句，并拼接成完整的诗。然后，函数为诗句中的所有字符创建字符到索引的映射字典和索引到字符的反向映射字典。接着，将每个诗句转化为由字符索引构成的向量，并根据设定的最大长度进行填充或截断。

采用独特字符标记标题和内容的分隔和标记诗句的结束，进而加深模型对这些关键位置的理解。

### 3.2 模型设置
利用简单的lstm模型进行训练，模型如下：
```python
class RNN_model(nn.Module):
    def __init__(self, vocab_len ,word_embedding, embedding_dim, lstm_hidden_dim):
        super(RNN_model,self).__init__()
        self.word_embedding_lookup = word_embedding
        self.vocab_length = vocab_len  #可选择的单词数目 或者说 word embedding层的word数目
        self.word_embedding_dim = embedding_dim
        self.lstm_dim = lstm_hidden_dim
        self.rnn_lstm = nn.LSTM(input_size=embedding_dim, 
                                 hidden_size=lstm_hidden_dim, 
                                 num_layers=2,
                                 batch_first=True)

        self.fc = nn.Linear(self.lstm_dim, self.vocab_length)
        nn.init.xavier_uniform_(self.fc.weight)
    def forward(self,sentence,batch_size,is_test = False):
        batch_input = self.word_embedding_lookup(sentence).view(batch_size,-1,self.word_embedding_dim)
        hidden = torch.zeros(2, batch_size, self.lstm_dim).to(next(self.parameters()).device)
        cell = torch.zeros(2, batch_size, self.lstm_dim).to(next(self.parameters()).device)
        output, _ = self.rnn_lstm(batch_input, (hidden, cell))

        out = output.contiguous().view(-1,self.lstm_dim)
        out = self.fc(out)   #out.size: (batch_size * sequence_length ,vocab_length)
        if is_test:
            #测试阶段(或者说生成诗句阶段)使用
            prediction = out[ -1, : ].view(1,-1)
            output = prediction
        else:
            #训练阶段使用
           output = out
        return output
```
### 3.3 模型训练
由于模型的主要目标是根据上下文生成古诗文，在训练中，我利用向量化后的数据的前n-1个字符作为x，利用后n-1个字符作为目标数据y。

我按照朝代（唐、宋）划分数据集进行分别训练，历经30个epoch训练后获得两个最终的模型。

### 3.4 训练结果
训练了多个模型 其中宋诗模型表现还可以，可能是因为数据相对更多一些

测试代码：
```python
print(gen_poem("春日访王丞相X"))
print(gen_poem("江岸别杜甫X"))
print(gen_poem("送白居易X"))
print(gen_poem("月见X"))
print(gen_poem("与李白会诗X"))
print(gen_poem("北山雪X"))
```
宋诗模型结果：

#### 春日访王丞相

春风吹雨过，春色满春风。花柳花如锦，春风吹柳花。

#### 江岸别杜甫

江南江北路，江北见山川。一舸归船去，孤帆过岭云。

#### 送白居易

一别三年别，今年一再归。一年归故国，一别一年秋。

#### 月见

一片飞来月，朣朣照夜光。月明蟾影碎，风入玉楼斜。

#### 与李白会诗

一片青山一点尘，一年春色满人家。春风吹尽春风急，不见春风一片春。

#### 北山雪

山中有佳处，山色自相依。山色无人到，山深不见梅。

### 3.5 模型反思
可以看到，经过大量的数据训练，基础的lstm模型已经能根据标题写出似是而非的诗句，但是可能对于给出的标题的理解程度不高，同时出现一首诗中单字多次出现的情况。

LSTM生成的序列通常是基于当前状态的线性递推，而缺乏足够的上下文理解，导致在生成诗句时，容易出现词汇重复或不合适的词汇使用。

LSTM在理解上下文关联、捕捉复杂语义及生成更连贯、自然的诗句方面相较于更强大的模型（如GPT-2）较为有限

## 4. 基于中文GPT2微调的古诗生成模型

### 4.1 模型选择

考虑到lstm模型的局限性，我最终决定采用GPT-2模型来进一步进行模型的训练。我最终采用一个已经经过预训练的中文GPT-2进行训练和微调，模型来自transformer库。

### 4.2 模型调用
由于网络访问存在问题，我将原模型下载下来，利用transformer库进行调用。
```python
from transformers import BertTokenizer, GPT2LMHeadModel,TextGenerationPipeline
local_model_path = './gpt2-chn'
tokenizer = BertTokenizer.from_pretrained(local_model_path)
model = GPT2LMHeadModel.from_pretrained(local_model_path)
```
GPT2LMHeadModel是用于语言建模任务的GPT-2模型，它根据输入的token序列生成预测的下一个token，正好符合我的希望。

### 4.3 数据处理
由于模型自带比较全面的tokenizer，我只需要对诗句进行简单处理即可，利用冒号分割标题和内容，并整合在一起。

随后，我使用了datasets库来处理和标记化诗歌数据，并将其准备好以供模型训练。首先使用预先加载的tokenizer将每个诗句进行标记化，形成输入序列input_ids。在数据的标签问题上，我直接将将input_ids复制给labels，使得标签和输入序列相同，因为GPT-2模型是基于自回归生成的，因此当前输入的下一个词（字符）是目标。随后我将数据转换为pytorch形式。
```python
from datasets import Dataset
max_length=128
def tokenize_function(examples):
    encodings = tokenizer(examples['fullpoem'], padding="max_length", truncation=True, max_length=max_length, return_tensors="pt")
    input_ids = encodings['input_ids']
    labels = input_ids.clone()
    encodings['labels'] = labels
    return encodings
dataset = Dataset.from_pandas(df_poems)
tokenized_dataset = dataset.map(tokenize_function, batched=True)
tokenized_dataset.set_format(type='torch', columns=['labels','input_ids', 'attention_mask'])
```






