# 项目报告：基于GPT2中文大模型的古诗词生成模型
孙志钢 2021200123
## 1. 研究背景和研究目标
### 1.1 研究背景
古诗词作为中国传统文化的瑰宝，具有深厚的历史背景和广泛的文化影响力。它们不仅是文学艺术的表现形式，也承载了丰富的哲学思想、历史故事、情感表达等内容。

随着人工智能和自然语言处理（NLP）技术的飞速发展，深度学习模型，特别是基于神经网络的生成模型，已经在多个领域取得了突破。近年来，GPT系列模型、BERT、Transformer等技术的出现使得语言生成任务，尤其是文学创作，进入了新的阶段。

尽管现代技术在诗词创作方面已取得一定进展，但由于古诗词具有独特的韵律、对仗、意境等要求，古诗词的自动生成依然面临诸多挑战。许多早期的模型未能很好地解决语言流畅性、文化底蕴与创意之间的平衡问题。因此，如何利用深度学习技术生成具有较高文学价值的古诗词，成为了一个重要的研究方向。

### 1.2 研究目的
目前的自动古诗词生成模型面临的问题包括语法结构不准确、韵律不和谐、情感表达不到位等。研究的目的之一是通过改进模型架构，优化训练数据，增强模型对古诗词特征的学习能力，提高生成诗词的质量与准确性，使其更具文学性与艺术性。

## 2. 数据准备
本项目采用中华诗词数据库，数据库链接：[诗词数据库](https://github.com/chinese-poetry/chinese-poetry)

此数据库是最全的中华古典文集数据库，包含 5.5 万首唐诗、26 万首宋诗、2.1 万首宋词和其他古典文集。诗人包括唐宋两朝近 1.4 万古诗人，和两宋时期 1.5 千古词人。数据来源于互联网。

本项目提取其中的5.5 万首唐诗、26 万首宋诗、2.1 万首宋词作为训练集进行训练，利用数据库进行基本操作后转化为csv。详细路径见[数据](./src/data_path.md)

## 3. LSTM模型尝试
本次首先采用LSTM模型进行初步的生成模型训练，主要代码见src下old version文件夹

### 3.1 数据处理
由于数据库主要为繁体字，首先调用hanziconv库进行简单的简繁转换。

首先，利用函数读取并清洗数据，去除缺失值，过滤掉包含禁用词或过长的诗句，并拼接成完整的诗。然后，函数为诗句中的所有字符创建字符到索引的映射字典和索引到字符的反向映射字典。接着，将每个诗句转化为由字符索引构成的向量，并根据设定的最大长度进行填充或截断。

采用独特字符标记标题和内容的分隔和标记诗句的结束，进而加深模型对这些关键位置的理解。

### 3.2 模型设置
利用简单的lstm模型进行训练，模型如下：
```python
class RNN_model(nn.Module):
    def __init__(self, vocab_len ,word_embedding, embedding_dim, lstm_hidden_dim):
        super(RNN_model,self).__init__()
        self.word_embedding_lookup = word_embedding
        self.vocab_length = vocab_len  #可选择的单词数目 或者说 word embedding层的word数目
        self.word_embedding_dim = embedding_dim
        self.lstm_dim = lstm_hidden_dim
        self.rnn_lstm = nn.LSTM(input_size=embedding_dim, 
                                 hidden_size=lstm_hidden_dim, 
                                 num_layers=2,
                                 batch_first=True)

        self.fc = nn.Linear(self.lstm_dim, self.vocab_length)
        nn.init.xavier_uniform_(self.fc.weight)
    def forward(self,sentence,batch_size,is_test = False):
        batch_input = self.word_embedding_lookup(sentence).view(batch_size,-1,self.word_embedding_dim)
        hidden = torch.zeros(2, batch_size, self.lstm_dim).to(next(self.parameters()).device)
        cell = torch.zeros(2, batch_size, self.lstm_dim).to(next(self.parameters()).device)
        output, _ = self.rnn_lstm(batch_input, (hidden, cell))

        out = output.contiguous().view(-1,self.lstm_dim)
        out = self.fc(out)   #out.size: (batch_size * sequence_length ,vocab_length)
        if is_test:
            #测试阶段(或者说生成诗句阶段)使用
            prediction = out[ -1, : ].view(1,-1)
            output = prediction
        else:
            #训练阶段使用
           output = out
        return output
```
### 3.3 模型训练
由于模型的主要目标是根据上下文生成古诗文，在训练中，我利用向量化后的数据的前n-1个字符作为x，利用后n-1个字符作为目标数据y。

我按照朝代（唐、宋）划分数据集进行分别训练，历经30个epoch训练后获得两个最终的模型。

### 3.4 训练结果
唐诗模型：







